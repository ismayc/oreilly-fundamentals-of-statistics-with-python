---
title: "Walkthroughs and Exercises for *Fundamentals of Statistics with Python*"
author: "Dr. Chester Ismay"
format: 
  html:
    fig-width: 10
    fig-height: 5
  ipynb:
    fig-width: 10
    fig-height: 5  
engine: knitr
---

```{python, eval=FALSE}
# Install the packages directly if needed with pip
# Check the repo's README below for more information
!pip install numpy pandas scipy matplotlib seaborn statsmodels scikit-learn jupyter
```


```{python}
import pandas as pd

# Display all columns
pd.set_option('display.max_columns', None)

# Display all outputs from each cell
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
```


# Week 1

## Walkthrough 1.1: Getting Started

### Setting Up the Python Environment

If you haven't already installed Python, Jupyter, and the necessary packages, there are instructions on the course repo in the README to do so [here](https://github.com/ismayc/oreilly-fundamentals-of-statistics-with-python/blob/main/README.md). 

If you aren't able to do this on your machine, you may want to check out [Google Colab](https://colab.research.google.com/). It's a free service that allows you to run Jupyter notebooks in the cloud. Alternatively, I've set up some temporary notebooks on Binder ([![()](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ismayc/oreilly-fundamentals-of-statistics-with-python/tree/main/HEAD)) that you can work with online as well.

```{python}
# Importing libraries/modules and aliasing them as needed
import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.preprocessing import MinMaxScaler, StandardScaler
```

### Exploring a dataset

```{python}
# Load in the dataset
data_dev_survey = pd.read_csv("data_dev_survey.csv")
```

```{python}
# Display information about the DataFrame
data_dev_survey.info()
```

### Performing basic statistical functions using NumPy, Pandas, and SciPy.

#### Using NumPy

```{python}
# Calculate the mean of the years_code_pro column
np.mean(data_dev_survey['years_code_pro'])
```

```{python}
# Calculate the median of the work_exp column
np.median(data_dev_survey['work_exp'])
```

```{python}
# Calculate the standard deviation of the converted_comp_yearly column
np.std(data_dev_survey['converted_comp_yearly'])
```

#### Using Pandas

```{python}
# Display summary statistics on the numeric columns of the DataFrame
data_dev_survey.describe()
```

#### Using SciPy

```{python}
# Separate the data into two groups
using_ai = data_dev_survey[data_dev_survey['plans_to_use_ai'] == 'Using']\
            ['converted_comp_yearly']
plan_to_use_ai = data_dev_survey[data_dev_survey['plans_to_use_ai'] == 'Plan to use']\
            ['converted_comp_yearly']

# Perform the t-test
t_stat, p_value = stats.ttest_ind(using_ai, plan_to_use_ai, equal_var=False)

# Print the results
print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")
```


## Exercise 1.1: Getting Started

### Setting Up the Python Environment

If you ran the `# Importing libraries and aliasing them` code above, you should 
be good to proceed here. If not, scroll up and run it.

### Exploring a dataset

```{python}
# Load in the coffee_quality dataset
coffee_quality = pd.read_csv("coffee_quality.csv")
```

```{python}
# Display information about the DataFrame
coffee_quality.info()
```

### Performing basic statistical functions using NumPy, Pandas, and SciPy.

#### Using NumPy

```{python}
# Calculate the mean of the aroma column
np.mean(coffee_quality['aroma'])
```

```{python}
# Calculate the median of the total_cup_points column
np.median(coffee_quality['total_cup_points'])
```

```{python}
# Calculate the standard deviation of the moisture_percentage column
np.std(coffee_quality['moisture_percentage'])
```

#### Using Pandas

```{python}
# Display summary statistics on the numeric columns of the DataFrame
coffee_quality.describe()
```

#### Using SciPy

```{python}
# Focus on only Asia and North America entries

# Separate the data into two groups
asian = coffee_quality\
            [coffee_quality['continent_of_origin'] == 'Asia']\
            ['total_cup_points']
north_american = coffee_quality\
            [coffee_quality['continent_of_origin'] == 'North America']\
            ['total_cup_points']

# Perform the t-test
t_stat, p_value = stats.ttest_ind(asian, north_american, equal_var=False)

# Print the results
print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")
```



---

## Walkthrough 1.2: Data Summarizing

### Compute and interpret measures of central tendency 

```{python}
# Calculate center statistics for years_code_pro
data_dev_survey['years_code_pro'].mean()
data_dev_survey['years_code_pro'].median()
data_dev_survey['years_code_pro'].mode()

# To extract just the value for mode
data_dev_survey['years_code_pro'].mode()[0]
```

### Compute and interpret measures of variation

```{python}
# Calculate spread statistics for years_code_pro
range_years_code_pro = data_dev_survey['years_code_pro'].max() \
                        - data_dev_survey['years_code_pro'].min()
range_years_code_pro

data_dev_survey['years_code_pro'].var(ddof=1)
data_dev_survey['years_code_pro'].std(ddof=1)
```

```{python}
# Calculate the five-number summary for 'years_code_pro'
min_years_code_pro = data_dev_survey['years_code_pro'].min()
q1_years_code_pro = data_dev_survey['years_code_pro'].quantile(0.25)
median_years_code_pro = data_dev_survey['years_code_pro'].median()
q3_years_code_pro = data_dev_survey['years_code_pro'].quantile(0.75)
max_years_code_pro = data_dev_survey['years_code_pro'].max()

# Print them out
print(f"Five-Number Summary for years_code_pro:")
print(f"Minimum: {min_years_code_pro}")
print(f"First Quartile (Q1): {q1_years_code_pro}")
print(f"Median (Q2): {median_years_code_pro}")
print(f"Third Quartile (Q3): {q3_years_code_pro}")
print(f"Maximum: {max_years_code_pro}")
```



## Exercise 1.2: Data Summarizing

```{python}
# Calculate center statistics for 'aroma'
mean_aroma = coffee_quality['aroma'].mean()
median_aroma = coffee_quality['aroma'].median()
mode_aroma = coffee_quality['aroma'].mode()[0]

print(f"Mean of aroma: {mean_aroma}")
print(f"Median of aroma: {median_aroma}")
print(f"Mode of aroma: {mode_aroma}")
```

```{python}
# Calculate spread statistics for 'aroma'
range_aroma = coffee_quality['aroma'].max() - coffee_quality['aroma'].min()
variance_aroma = coffee_quality['aroma'].var(ddof=1)  # Sample variance
std_dev_aroma = coffee_quality['aroma'].std(ddof=1)  # Sample standard deviation

print(f"Range of aroma: {range_aroma}")
print(f"Variance of aroma: {variance_aroma}")
print(f"Standard Deviation of aroma: {std_dev_aroma}")
```

```{python}
# Calculate the five-number summary for 'aroma'
min_aroma = coffee_quality['aroma'].min()
q1_aroma = coffee_quality['aroma'].quantile(0.25)
median_aroma = coffee_quality['aroma'].median()
q3_aroma = coffee_quality['aroma'].quantile(0.75)
max_aroma = coffee_quality['aroma'].max()

print(f"Five-Number Summary for aroma:")
print(f"Minimum: {min_aroma}")
print(f"First Quartile (Q1): {q1_aroma}")
print(f"Median (Q2): {median_aroma}")
print(f"Third Quartile (Q3): {q3_aroma}")
print(f"Maximum: {max_aroma}")
```


---

## Walkthrough 1.3: Cleaning and Preparing Data with Pandas

```{python}
# Histogram for years_code_pro
sns.histplot(data_dev_survey['years_code_pro']);
plt.title('Histogram of Years of Coding Experience');
plt.xlabel('Years of Coding Experience');
plt.ylabel('Frequency');
plt.show()
```

```{python}
# Set the default figure size for all plots (unless specified)
plt.rcParams['figure.figsize'] = (10, 5)
```



```{python}
# Box plot for work_exp
sns.boxplot(y=data_dev_survey['work_exp']);
plt.title('Box Plot of Work Experience');
plt.ylabel('Work Experience (years)');
plt.show()
```

```{python}
# Can also make it horizontal in orientation
sns.boxplot(x=data_dev_survey['work_exp']);
plt.title('Box Plot of Work Experience');
plt.xlabel('Work Experience (years)');
plt.show()
```


```{python}
# Scatter plot for years_code_pro vs. converted_comp_yearly
sns.scatterplot(x=data_dev_survey['years_code_pro'], 
                y=data_dev_survey['converted_comp_yearly']);
plt.title('Scatter Plot of Years of Coding Experience vs. Yearly Compensation');
plt.xlabel('Years of Coding Experience');
plt.ylabel('Yearly Compensation');
plt.show()
```

## Exercise 1.3: Cleaning and Preparing Data with Pandas

```{python}
# Histogram for acidity
sns.histplot(coffee_quality['acidity']);
plt.title('Histogram of Coffee Acidity');
plt.xlabel('Acidity');
plt.ylabel('Frequency');
plt.show()
```

```{python}
# Box plot for body
sns.boxplot(y=coffee_quality['body']);
plt.title('Box Plot of Coffee Body');
plt.ylabel('Body');
plt.show()
```

```{python}
# Set the Seaborn style to 'darkgrid'
# This sets the color of the points to dark with a light grid background
sns.set_style("darkgrid")

# Scatter plot for body vs. acidity
sns.scatterplot(x=coffee_quality['body'], y=coffee_quality['acidity']);
plt.title('Scatter Plot of Body vs. Acidity');
plt.xlabel('Body');
plt.ylabel('Acidity');
plt.show()
```


---

## Walkthrough 1.4: Sampling Distribution Generation

```{python}
# Think of our data as a population to draw from
population = data_dev_survey['converted_comp_yearly'].dropna().values

# Generate a large sample from the 'converted_comp_yearly' column
# Parameters
sample_size = 50
num_samples = 1000

# Set a seed to make code reproducible
np.random.seed(2024)

# Simulate sampling distribution of the mean
sample_means = []
for _ in range(num_samples):
    sample = np.random.choice(population, sample_size)
    sample_means.append(np.mean(sample))

# Plot the sampling distribution of the sample means
plt.hist(sample_means, bins=30, edgecolor='k', alpha=0.7);
plt.title('Sampling Distribution of the Mean (Sample Size = 50)');
plt.xlabel('Sample Mean');
plt.ylabel('Frequency');
plt.xticks(rotation=30);
# Adjust layout so labels are not cut off
plt.tight_layout();
plt.show()
```


## Exercise 1.4: Sampling Distribution Generation

```{python}
# Think of our aroma data as a population to draw from
population = coffee_quality['aroma'].dropna().values

# Parameters
sample_size = 50
num_samples = 1000

# Set random seed
np.random.seed(2024)

# Simulate sampling distribution of the mean
sample_means = []
for _ in range(num_samples):
    sample = np.random.choice(population, sample_size)
    sample_means.append(np.mean(sample))

# Plot the sampling distribution of the sample means
plt.hist(sample_means, bins=30, edgecolor='k', alpha=0.7);
plt.title('Sampling Distribution of the Mean (Sample Size = 50)');
plt.xlabel('Sample Mean');
plt.ylabel('Frequency');
plt.xticks(rotation=30);
# Adjust layout so labels are not cut off
plt.tight_layout();
plt.show()
```


---

# Week 2

## Walkthrough 2.1: Advanced Plots

```{python}
# Select only numeric columns
numeric_columns = data_dev_survey.select_dtypes(include=[np.number])

# Calculate the correlation matrix for numeric columns
correlation_matrix = numeric_columns.corr()

# Heatmap for correlation matrix
plt.figure(figsize=(10, 8));
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f'); 
plt.title('Heatmap of Correlation Matrix');
plt.show()
```

```{python}
# Pair plot for selected variables 'years_code_pro', 'work_exp', 'converted_comp_yearly'
sns.pairplot(data_dev_survey, 
             vars=['years_code_pro', 'work_exp', 'converted_comp_yearly']);
plt.suptitle('Pair Plot of Selected Variables', y=1);
plt.show()
```

```{python}
# Time series plot
data_dev_survey['survey_completion_date'] = \
  pd.to_datetime(data_dev_survey['survey_completion_date'])

# Count the number of surveys completed each day
daily_counts = data_dev_survey['survey_completion_date'].value_counts().sort_index()

# Plot the counts as a line chart
plt.figure(figsize=(12, 6));
plt.plot(daily_counts.index, daily_counts.values, marker='o');
plt.title('Number of Surveys Completed Each Day in May 2023');
plt.xlabel('Day Completed');
plt.ylabel('Number of Surveys');
plt.xticks(rotation=30);
plt.grid(True);
plt.tight_layout();
plt.show()
```

## Exercise 2.1: Advanced Plots

```{python}
# Select only numeric columns
numeric_columns = coffee_quality.select_dtypes(include=[np.number])

# Drop the 'clean_cup' and 'sweetness' columns since they are always 10
numeric_columns = numeric_columns.drop(columns=['clean_cup', 'sweetness'], 
                                       errors='ignore')

# Calculate the correlation matrix for numeric columns
correlation_matrix = numeric_columns.corr()

# Heatmap for correlation matrix
plt.figure(figsize=(10, 8));
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f');
plt.title('Heatmap of Correlation Matrix');
plt.show()
```

```{python}
# Pair plot for selected variables ['aroma', 'acidity', 'body']
sns.pairplot(coffee_quality, vars=['aroma', 'acidity', 'body']);
plt.suptitle('Pair Plot of Selected Variables', y=1);
plt.show()
```


- **Datetime Accessor `.dt`:**
   - The `.dt` accessor is used with pandas Series that contain datetime objects.
   - It provides access to datetime-specific attributes and methods.

- **Convert to a Period with Monthly Frequency:**
   - The method `.to_period('M')` converts each datetime value into a **Period** object with a monthly frequency.
   - The argument `'M'` stands for "month". This means that each date is represented as a period corresponding to its month and year.
   - For example, if `grading_date` is `2023-03-15`, converting it with `.to_period('M')` will result in a period like `2023-03`.


```{python}
# Plot the mean total_cup_points for each grading_date

# Convert to datetime
coffee_quality['grading_date'] = pd.to_datetime(coffee_quality['grading_date'])

# Extract the month and year from the grading_date column
coffee_quality['month'] = coffee_quality['grading_date'].dt.to_period('M')

# Aggregate the mean total_cup_points by month
monthly_mean = coffee_quality.groupby('month')['total_cup_points'].mean()

# Plot the mean total_cup_points by month as a line chart
plt.figure(figsize=(12, 6));
plt.plot(monthly_mean.index.astype(str), monthly_mean.values, marker='o');
plt.title('Mean Total Cup Points by Month Graded');
plt.xlabel('Month');
plt.ylabel('Mean Total Cup Points');
plt.xticks(rotation=45);
plt.grid(True);
plt.tight_layout();
plt.show()
```

---

## Walkthrough 2.2: EDA

### Data Cleaning

```{python}
# Check for missing values
data_dev_survey.isnull().sum()

# Remove duplicates if any
data_dev_survey = data_dev_survey.drop_duplicates()
data_dev_survey.info()
```

### Data Visualization

```{python}
# Histogram for years_code_pro
sns.histplot(data_dev_survey['years_code_pro']);
plt.title('Histogram of Years of Coding Experience');
plt.xlabel('Years of Coding Experience');
plt.ylabel('Frequency');
plt.show()

# Box plot for work_exp
sns.boxplot(y=data_dev_survey['work_exp']);
plt.title('Box Plot of Work Experience');
plt.ylabel('Work Experience (years)');
plt.show()

# Scatter plot for years_code_pro vs. converted_comp_yearly
sns.scatterplot(x=data_dev_survey['years_code'],
                y=data_dev_survey['converted_comp_yearly']);
plt.title('Scatter Plot of Years of Coding Experience vs. Yearly Compensation');
plt.xlabel('Years of Coding Experience');
plt.ylabel('Yearly Compensation');
plt.show()
```


### Summary Statistics

```{python}
# Calculate summary statistics
data_dev_survey.describe()
```


## Exercise 2.2: EDA

### Data Cleaning

```{python}
# Check for missing values
coffee_quality.isnull().sum()

# Remove duplicates if any
coffee_quality = coffee_quality.drop_duplicates()
```


### Data Visualization

```{python}
# Histogram for aroma
sns.histplot(coffee_quality['aroma']);
plt.title('Histogram of Coffee Aroma');
plt.xlabel('Aroma');
plt.ylabel('Frequency');
plt.show()

# Box plot for acidity
sns.boxplot(y=coffee_quality['acidity']);
plt.title('Box Plot of Coffee Acidity');
plt.ylabel('Acidity');
plt.show()

# Scatter plot for aroma vs. total_cup_points
sns.scatterplot(x=coffee_quality['aroma'], 
                y=coffee_quality['total_cup_points']);
plt.title('Scatter Plot of Aroma vs. Total Cup Points');
plt.xlabel('Aroma');
plt.ylabel('Total Cup Points');
plt.show()
```



### Summary Statistics

```{python}
# Calculate summary statistics
coffee_quality.describe()
```


---

## Walkthrough 2.3: Data Preprocessing

### Inspect the Data after Loading

```{python}
# Display basic information about the dataset
data_dev_survey.info()

# Display the first few rows of the dataset
data_dev_survey.head()
```

### Handle Missing Values

```{python}
# Check for missing values
data_dev_survey.isnull().sum()

# Make a copy of the dataset for imputation
data_dev_survey_imputed = data_dev_survey.copy()

# Select only numeric columns
numeric_columns = data_dev_survey_imputed\
  .select_dtypes(include=[np.number])\
  .columns

# Fill missing values in numeric columns with the median
data_dev_survey_imputed[numeric_columns] = data_dev_survey_imputed[numeric_columns]\
  .fillna(data_dev_survey_imputed[numeric_columns]\
  .median())

# Display the first few rows of the imputed dataset
data_dev_survey_imputed.head()
```

### Handling Outliers

```{python}
# Select only numeric columns
numeric_data = data_dev_survey_imputed.select_dtypes(include=[np.number])

# Identify outliers using IQR
Q1 = numeric_data.quantile(0.25)
Q3 = numeric_data.quantile(0.75)
IQR = Q3 - Q1
outliers = numeric_data[(numeric_data < (Q1 - 1.5 * IQR)) \
                      | (numeric_data > (Q3 + 1.5 * IQR))]
print(outliers)

# Remove outliers
data_dev_survey_imputed = data_dev_survey_imputed[~(
    (numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))
  ).any(axis=1)]
```

### Data Transformation

```{python}
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Normalization on converted_comp_yearly
scaler = MinMaxScaler()
data_dev_survey_imputed[['converted_comp_yearly']] = scaler.fit_transform(
  data_dev_survey_imputed[['converted_comp_yearly']]
)

# Standardization on work_exp
scaler = StandardScaler()
data_dev_survey_imputed[['work_exp']] = scaler.fit_transform(
  data_dev_survey_imputed[['work_exp']]
)

# Encoding categorical variables (country)
data_dev_survey_imputed = pd.get_dummies(data_dev_survey_imputed, 
                                         columns=['country'])
```

### Data Visualizations on Preprocessed Data

```{python}
# Histogram for years_code_pro
sns.histplot(data_dev_survey_imputed['years_code_pro']);
plt.title('Histogram of Years of Coding Experience');
plt.xlabel('Years of Coding Experience');
plt.ylabel('Frequency');
plt.show()

# Box plot for work_exp
sns.boxplot(y=data_dev_survey_imputed['work_exp']);
plt.title('Box Plot of Work Experience');
plt.ylabel('Work Experience (years)');
plt.show()

# Scatter plot for years_code_pro vs. converted_comp_yearly
sns.scatterplot(x=data_dev_survey_imputed['years_code_pro'],
                y=data_dev_survey_imputed['converted_comp_yearly']);
plt.title('Scatter Plot of Years of Coding Experience vs. Yearly Compensation');
plt.xlabel('Years of Coding Experience');
plt.ylabel('Yearly Compensation');
plt.show()
```


## Exercise 2.3: Data Preprocessing

### Inspect the Data after Loading

```{python}
# Display basic information about the dataset
coffee_quality.info()

# Display the first few rows of the dataset
coffee_quality.head()
```

### Handle Missing Values

```{python}
# Check for missing values
coffee_quality.isnull().sum()

# Make a copy of the dataset for imputation
coffee_quality_imputed = coffee_quality.copy()

# Select only numeric columns
numeric_columns = coffee_quality_imputed.select_dtypes(include=[np.number]).columns

# Fill missing values in numeric columns with the median
coffee_quality_imputed[numeric_columns] = coffee_quality_imputed[numeric_columns]\
  .fillna(coffee_quality_imputed[numeric_columns].median())

# Display the first few rows of the imputed dataset
coffee_quality_imputed.head()
```

### Handle Outliers

```{python}
# Select only numeric columns
numeric_data = coffee_quality_imputed.select_dtypes(include=[np.number])

# Identify outliers using IQR
Q1 = numeric_data.quantile(0.25)
Q3 = numeric_data.quantile(0.75)
IQR = Q3 - Q1
outliers = numeric_data[(numeric_data < (Q1 - 1.5 * IQR)) \
                      | (numeric_data > (Q3 + 1.5 * IQR))]
print(outliers)

# Remove outliers
coffee_quality_imputed = coffee_quality_imputed[~(
    (numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))
  ).any(axis=1)]
```

### Data Transformation

```{python}
# Normalization on total_cup_points
scaler = MinMaxScaler()
coffee_quality_imputed[['total_cup_points']] = \
  scaler.fit_transform(coffee_quality_imputed[['total_cup_points']])
# Can use this instead
coffee_quality_imputed.loc[:, ['total_cup_points']] = \
  scaler.fit_transform(coffee_quality_imputed[['total_cup_points']])

# Standardization on acidity
scaler = StandardScaler()
coffee_quality_imputed[['acidity']] = scaler.fit_transform(
  coffee_quality_imputed[['acidity']]
)
# Or this
coffee_quality_imputed.loc[:, ['acidity']] = \
  scaler.fit_transform(coffee_quality_imputed[['acidity']])

# Encoding categorical variables (country and continent of origin)
coffee_quality_imputed = pd.get_dummies(
  coffee_quality_imputed, 
  columns=['country_of_origin', 'continent_of_origin'])
```

### Data Visualizations on Preprocessed Data

```{python}
# Histogram for aroma
sns.histplot(coffee_quality_imputed['aroma']);
plt.title('Histogram of Coffee Aroma');
plt.xlabel('Aroma');
plt.ylabel('Frequency');
plt.show()

# Box plot for acidity
sns.boxplot(y=coffee_quality_imputed['acidity']);
plt.title('Box Plot of Coffee Acidity');
plt.ylabel('Acidity');
plt.show()

# Scatter plot for aroma vs. total_cup_points
sns.scatterplot(x=coffee_quality_imputed['aroma'], 
                y=coffee_quality_imputed['total_cup_points']);
plt.title('Scatter Plot of Aroma vs. Total Cup Points');
plt.xlabel('Aroma');
plt.ylabel('Total Cup Points');
plt.show()
```



---

## Walkthrough 2.4: Correlations

### Correlation Matrix

```{python}
# Select only numeric columns
numeric_columns = data_dev_survey.select_dtypes(include=[np.number])

# Calculate the correlation matrix
correlation_matrix = numeric_columns.corr()

# Display the correlation matrix
correlation_matrix
```

### Visualize Correlations

```{python}
# Heatmap for correlation matrix
plt.figure(figsize=(10, 8));
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f');
plt.title('Heatmap of Correlation Matrix');
plt.show()
```

### Create Scatter Plots for Meaningful Correlations

```{python}
# Scatter plot for years_code_pro vs. years_code
sns.scatterplot(x=data_dev_survey['years_code_pro'], 
                y=data_dev_survey['years_code']);
plt.title('Scatter Plot of Years of Professional Coding Experience vs. Years of Professional Coding Experience');
plt.xlabel('Years of Professional Coding Experience');
plt.ylabel('Years of Coding Experience');
plt.show()

# Scatter plot for work_exp vs. years_code_pro
sns.scatterplot(x=data_dev_survey['work_exp'], y=data_dev_survey['years_code_pro']);
plt.title('Scatter Plot of Work Experience vs. Years of Professional Coding Experience');
plt.xlabel('Work Experience');
plt.ylabel('Years of Professional Coding Experience');
plt.show()
```

## Exercise 2.4: Correlations

### Correlation Matrix

```{python}
# Select only numeric columns
numeric_columns = coffee_quality.select_dtypes(include=[np.number])

# Remove clean_cup and sweetness too
numeric_columns = numeric_columns.drop(columns=['clean_cup', 'sweetness'], 
                                       errors='ignore')

# Calculate the correlation matrix
correlation_matrix = numeric_columns.corr()

# Display the correlation matrix
correlation_matrix
```


### Visualize Correlations

```{python}
# Heatmap for correlation matrix
plt.figure(figsize=(10, 8));
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f');
plt.title('Heatmap of Correlation Matrix');
plt.show()
```

### Create Scatter Plots for Meaningful Correlations

```{python}
# Scatter plot for flavor vs. total_cup_points
sns.scatterplot(x=coffee_quality['flavor'], 
                y=coffee_quality['total_cup_points']);
plt.title('Scatter Plot of Flavor vs. Total Cup Points');
plt.xlabel('Flavor');
plt.ylabel('Total Cup Points');
plt.show()

# Scatter plot for overall vs. total_cup_points
sns.scatterplot(x=coffee_quality['overall'], 
                y=coffee_quality['total_cup_points']);
plt.title('Scatter Plot of Overall vs. Total Cup Points');
plt.xlabel('Overall');
plt.ylabel('Total Cup Points');
plt.show()
```


---

# Week 3

## Walkthrough 3.1: Simulating Distributions

### Simulating Binomial

```{python}
# Simulate binomial distribution
binom_samples = np.random.binomial(n=10, p=0.5, size=10000)

# Plot histogram
sns.histplot(binom_samples, kde=False, bins=30);
plt.title('Binomial Distribution (n=10, p=0.5)');
plt.xlabel('Number of Successes');
plt.ylabel('Frequency');
plt.show()
```

### Simulating Normal

```{python}
# Simulate normal distribution
normal_samples = np.random.normal(loc=0, scale=1, size=10000)

# Plot histogram
sns.histplot(normal_samples, kde=True, bins=30);
plt.title('Normal Distribution (μ=0, σ=1)');
plt.xlabel('Value');
plt.ylabel('Frequency');
plt.show()
```


## Exercise 3.1: 

### Simulating Poisson

```{python}
# Simulate Poisson distribution with lambda (lam) parameter 3
poisson_samples = np.random.poisson(lam=3, size=10000)

# Plot histogram
sns.histplot(poisson_samples, kde=False, bins=30);
plt.title('Poisson Distribution (λ=3)');
plt.xlabel('Number of Events');
plt.ylabel('Frequency');
plt.show()
```


### Simulating Exponential

```{python}
# Simulate exponential distribution with scale parameter 1
exponential_samples = np.random.exponential(scale=1, size=10000)

# Plot histogram
sns.histplot(exponential_samples, kde=True, bins=30);
plt.title('Exponential Distribution (λ=1)');
plt.xlabel('Value');
plt.ylabel('Frequency');
plt.show()
```

---

## Walkthrough 3.2: t-tests

```{python}
# One-sample t-test checking for evidence that mu compensation > 85000
salary_mean = 85000
t_stat, p_value = stats.ttest_1samp(
  data_dev_survey['converted_comp_yearly'].dropna(), 
  popmean = salary_mean,
  alternative='greater')

print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")
```

```{python}
# Two-sample t-test comparing compensation across plans_to_use_ai groups
# Checking for a difference
using = data_dev_survey[data_dev_survey['plans_to_use_ai'] == 'Using']\
          ['converted_comp_yearly']\
          .dropna()
plan_to_use = data_dev_survey[data_dev_survey['plans_to_use_ai'] == 'Plan to use']\
          ['converted_comp_yearly']\
          .dropna()
t_stat, p_value = stats.ttest_ind(using, plan_to_use, equal_var=False)

print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")
```


## Exercise 3.2: t-tests

```{python}
# One-sample t-test checking for evidence that mu flavor < 7.8
flavor_mean = 7.8
t_stat, p_value = stats.ttest_1samp(
  coffee_quality['flavor'].dropna(), 
  flavor_mean,
  alternative='less')

print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")
```

```{python}
# Two-sample t-test checking for difference in Columbia and Brazil total_cup_points
colombia = coffee_quality[coffee_quality['country_of_origin'] == 'Colombia']\
                ['total_cup_points']\
                .dropna()
brazil = coffee_quality[coffee_quality['country_of_origin'] == 'Brazil']\
                ['total_cup_points'].\
                dropna()
t_stat, p_value = stats.ttest_ind(colombia, brazil, equal_var=False)

print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")
```


---

## Walkthrough 3.3: Comparative Tests

<!-- Also show EDA first to make guesses as to statistical significance -->

The `*groups` syntax unpacks the lists from the `groups` variable so that each list is passed as a separate argument to `stats.f_oneway`. This function then performs a one-way ANOVA test across the different groups.

```{python}
# Perform one-way ANOVA comparing compensation across different levels of
# remote_work
groups = data_dev_survey.groupby('remote_work')['converted_comp_yearly']\
            .apply(list)
f_stat, p_value = stats.f_oneway(*groups)

print(f"F-statistic: {f_stat}")
print(f"P-value: {p_value}")
```

```{python}
# Create a contingency table of employment and remote_work
contingency_table = pd.crosstab(data_dev_survey['employment'], 
                                data_dev_survey['remote_work'])

# Perform chi-square test
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)

print(f"Chi-Square Statistic: {chi2}")
print(f"P-value: {p}")
```


## Exercise 3.3: Comparative Tests

```{python}
# Perform one-way ANOVA comparing total_cup_points across country_of_origin
groups = coffee_quality.groupby('country_of_origin')['total_cup_points']\
            .apply(list)
f_stat, p_value = stats.f_oneway(*groups)

print(f"F-statistic: {f_stat}")
print(f"P-value: {p_value}")
```

```{python}
# Perform a chi-square test of independence for processing_method versus
# continent_of_origin

# Create a contingency table
contingency_table = pd.crosstab(coffee_quality['processing_method'],
                                coffee_quality['continent_of_origin'])

# Perform chi-square test
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)

print(f"Chi-Square Statistic: {chi2}")
print(f"P-value: {p}")
```

---

## Walkthrough 3.4: Non-Parametric Tests

```{python}
# Perform Mann-Whitney U Test comparing compensation for Remote and In-person
group1 = data_dev_survey[data_dev_survey['remote_work'] == 'Remote']\
            ['converted_comp_yearly']\
            .dropna()
group2 = data_dev_survey[data_dev_survey['remote_work'] == 'In-person']\
            ['converted_comp_yearly']\
            .dropna()
stat, p_value = stats.mannwhitneyu(group1, group2)

print(f"Mann-Whitney U Statistic: {stat}")
print(f"P-value: {p_value}")
```

```{python}
# Perform Kruskal-Wallis H Test comparing compensation across countries
groups = data_dev_survey.groupby('country')['converted_comp_yearly'].apply(list)
stat, p_value = stats.kruskal(*groups)

print(f"Kruskal-Wallis H Statistic: {stat}")
print(f"P-value: {p_value}")
```


## Exercise 3.4: Non-Parametric Tests

```{python}
# Perform Mann-Whitney U Test comparing total_cup_points for Guatemala
# and Honduras
group1 = coffee_quality\
    [coffee_quality['country_of_origin'] == 'Guatemala']\
    ['total_cup_points']\
    .dropna()
group2 = coffee_quality\
    [coffee_quality['country_of_origin'] == 'Honduras']\
    ['total_cup_points']\
    .dropna()
stat, p_value = stats.mannwhitneyu(group1, group2)

print(f"Mann-Whitney U Statistic: {stat}")
print(f"P-value: {p_value}")

```

```{python}
# Perform Kruskal-Wallis H Test comparing total_cup_points across 
# continent_of_origin
groups = coffee_quality.groupby('continent_of_origin')['total_cup_points'].apply(list)
stat, p_value = stats.kruskal(*groups)

print(f"Kruskal-Wallis H Statistic: {stat}")
print(f"P-value: {p_value}")
```

